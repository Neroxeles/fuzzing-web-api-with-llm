general:
  # choose a fitting name for the current test run
  name: "Demo"
  # the directory that contains all produced files
  output-dir: "/content/fuzzing-web-api-with-llm/output"

model:
  # choose one of the following possible LLM model checkpoints
  # - bigcode/starcoderbase-1b
  # - bigcode/starcoder
  # see https://huggingface.co/collections/bigcode/%E2%AD%90-starcoder-64f9bd5740eb5daaeb81dbec for more
  # and for StarCoder2 see https://huggingface.co/collections/bigcode/starcoder2-65de6da6e87db3383572be1a
  #TODO examples: how much RAM needs each checkpoint?
  # bigcode/starcoderbase-1b mit torch.bfloat16 -> 2666 MB.
  # bigcode/starcoderbase mit torch.bfloat16 -> ~32 GB.
  checkpoint: "bigcode/starcoder"
  # choose one of the following devices
  # - cpu (not implemented yet)
  # - cuda (recommended)
  device: "cuda"
  # choose a fitting device_map
  device-map: "/content/fuzzing-web-api-with-llm/configs/device-maps/starcoder/device_map-colab-A100.json"
  # define offload_folder
  offload-folder: "offload"

phase-i:
  oas-file: "/content/fuzzing-web-api-with-llm/OAS/experimental/restful_oas.yml"